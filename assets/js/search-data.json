{
  
    
        "post0": {
            "title": "Install and configure HTTPS certificate",
            "content": "Installing Nginx . sudo apt update sudo apt install nginx . Setting up virtual host . cd /etc/nginx/sites-available sudo rm default sudo nano proxy.conf . server { listen 80; server_name &lt;hostname&gt;; location / { proxy_pass http://localhost:6789; } } . cd /etc/nginx/sites-enabled sudo rm default sudo ln -s /etc/nginx/sites-available/proxy.conf /etc/nginx/sites-enabled/ . Activating virtual host . sudo service nginx restart . Ensure that your version of snapd is up to date . sudo snap install core; sudo snap refresh core . Install Certbot . sudo snap install --classic certbot . Prepare the Certbot command . sudo ln -s /snap/bin/certbot /usr/bin/certbot . Get and install your certificates . sudo certbot --nginx . Test automatic renewal . sudo certbot renew --dry-run .",
            "url": "https://rishiraj.github.io/blog/ubuntu/setup/2021/12/26/_12_27_Install_and_configure_HTTPS_certificate.html",
            "relUrl": "/ubuntu/setup/2021/12/26/_12_27_Install_and_configure_HTTPS_certificate.html",
            "date": " ‚Ä¢ Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Dropout and Batch Normalization",
            "content": "Introduction . There&#39;s so much more to Chicken Biryani than just the chicken. Surely chicken plays the most important role, but the rice, the masala (and the potato if you&#39;re a Bengali) are important too. Similarly, in deep learning, there&#39;re layers apart from the dense layers that plays different important roles. Let&#39;s discuss two of them. . import matplotlib.pyplot as plt plt.style.use(&#39;seaborn-whitegrid&#39;) # Set Matplotlib defaults plt.rc(&#39;figure&#39;, autolayout=True) plt.rc(&#39;axes&#39;, labelweight=&#39;bold&#39;, labelsize=&#39;large&#39;, titleweight=&#39;bold&#39;, titlesize=18, titlepad=10) plt.rc(&#39;animation&#39;, html=&#39;html5&#39;) . Dropout . Dropout is an important technique for regularization that only emerged recently, thanks to Geoffrey Hinton and works amazingly well. In deep learning we have one layer connecting to another layer and the values that go from one layer to the next are called activations. Dropout takes these activations and randomly set half of them to zero for every example we train our network on. Simply put, it iteratively takes half of the data that&#39;s flowing through our network and just drops it. Thus, our network can never rely on any given activation to be present because they might be dropped at any given iteration. So it is forced to learn a redundant representation for everything to make sure that at least some of the information remains. This makes the network robust and prevents overfitting all while making it act like taking an agreement of an ensemble of networks. . Load Dataset and Preprocess . import pandas as pd from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import make_column_transformer from sklearn.model_selection import GroupShuffleSplit from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras import callbacks spotify = pd.read_csv(&#39;/content/spotify.csv&#39;) X = spotify.copy().dropna() y = X.pop(&#39;track_popularity&#39;) artists = X[&#39;track_artist&#39;] features_num = [&#39;danceability&#39;, &#39;energy&#39;, &#39;key&#39;, &#39;loudness&#39;, &#39;mode&#39;, &#39;speechiness&#39;, &#39;acousticness&#39;, &#39;instrumentalness&#39;, &#39;liveness&#39;, &#39;valence&#39;, &#39;tempo&#39;, &#39;duration_ms&#39;] features_cat = [&#39;playlist_genre&#39;] preprocessor = make_column_transformer( (StandardScaler(), features_num), (OneHotEncoder(), features_cat), ) def group_split(X, y, group, train_size=0.75): splitter = GroupShuffleSplit(train_size=train_size) train, test = next(splitter.split(X, y, groups=group)) return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test]) X_train, X_valid, y_train, y_valid = group_split(X, y, artists) X_train = preprocessor.fit_transform(X_train) X_valid = preprocessor.transform(X_valid) y_train = y_train / 100 y_valid = y_valid / 100 input_shape = [X_train.shape[1]] print(&quot;Input shape: {}&quot;.format(input_shape)) . Input shape: [18] . Define and fit the original Model . model = keras.Sequential([ layers.Dense(128, activation=&#39;relu&#39;, input_shape=input_shape), layers.Dense(64, activation=&#39;relu&#39;), layers.Dense(1) ]) model.compile( optimizer=&#39;adam&#39;, loss=&#39;mae&#39;, ) history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=50, verbose=0, ) history_df = pd.DataFrame(history.history) history_df.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() print(&quot;Minimum Validation Loss: {:0.4f}&quot;.format(history_df[&#39;val_loss&#39;].min())) . Minimum Validation Loss: 0.1922 . Define and fit the Model with Dropout . model = keras.Sequential([ layers.Dense(128, activation=&#39;relu&#39;, input_shape=input_shape), layers.Dropout(0.3), layers.Dense(64, activation=&#39;relu&#39;), layers.Dropout(0.3), layers.Dense(1) ]) . model.compile( optimizer=&#39;adam&#39;, loss=&#39;mae&#39;, ) history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=50, verbose=0, ) history_df = pd.DataFrame(history.history) history_df.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() print(&quot;Minimum Validation Loss: {:0.4f}&quot;.format(history_df[&#39;val_loss&#39;].min())) . Minimum Validation Loss: 0.1879 . From the learning curves, we can see that the validation loss remains near a constant minimum even though the training loss continues to decrease. So we can see that adding dropout did prevent overfitting this time. Moreover, by making it harder for the network to fit spurious patterns, dropout may have encouraged the network to seek out more of the true patterns, possibly improving the validation loss some as well). . Batch Normalization . In deep learning it is a common practice to normalize the data on a similar scale before it goes to the network. We can easily achieve this through the scalers available from sklearn.preprocessing and has the effect of preventing the network from getting unstable with the weights being shifted in proportion to how large an activation the data produces. In a same way Batch Normalization normalizes the data on a similar scale when it&#39;s inside the network, just before or after the activation function of each hidden layer. It zero-centers and normalizes each input, then shifts and scales the result using one new parameter vector for shifting and another for scaling for each layer. This makes the network learn the optimal scale and mean of each of the layer&#39;s inputs. . Load Dataset and Preprocess . import pandas as pd concrete = pd.read_csv(&#39;/content/concrete.csv&#39;) df = concrete.copy() df_train = df.sample(frac=0.7, random_state=0) df_valid = df.drop(df_train.index) X_train = df_train.drop(&#39;CompressiveStrength&#39;, axis=1) X_valid = df_valid.drop(&#39;CompressiveStrength&#39;, axis=1) y_train = df_train[&#39;CompressiveStrength&#39;] y_valid = df_valid[&#39;CompressiveStrength&#39;] input_shape = [X_train.shape[1]] . Define and fit the original Model . model = keras.Sequential([ layers.Dense(512, activation=&#39;relu&#39;, input_shape=input_shape), layers.Dense(512, activation=&#39;relu&#39;), layers.Dense(512, activation=&#39;relu&#39;), layers.Dense(1), ]) model.compile( optimizer=&#39;sgd&#39;, # SGD is more sensitive to differences of scale loss=&#39;mae&#39;, metrics=[&#39;mae&#39;], ) history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=100, verbose=0, ) history_df = pd.DataFrame(history.history) history_df.loc[0:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() print((&quot;Minimum Validation Loss: {:0.4f}&quot;).format(history_df[&#39;val_loss&#39;].min())) . Minimum Validation Loss: nan . Define and fit the Model with Batch Normalization . model = keras.Sequential([ layers.BatchNormalization(input_shape=input_shape), layers.Dense(512, activation=&#39;relu&#39;), layers.BatchNormalization(), layers.Dense(512, activation=&#39;relu&#39;), layers.BatchNormalization(), layers.Dense(512, activation=&#39;relu&#39;), layers.BatchNormalization(), layers.Dense(1), ]) . model.compile( optimizer=&#39;sgd&#39;, loss=&#39;mae&#39;, metrics=[&#39;mae&#39;], ) EPOCHS = 100 history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=64, epochs=EPOCHS, verbose=0, ) history_df = pd.DataFrame(history.history) history_df.loc[0:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() print((&quot;Minimum Validation Loss: {:0.4f}&quot;).format(history_df[&#39;val_loss&#39;].min())) . Minimum Validation Loss: 3.9774 . We can see that adding batch normalization was a big improvement on the first attempt! By adaptively scaling the data as it passes through the network, batch normalization can let us train models on difficult datasets. .",
            "url": "https://rishiraj.github.io/blog/deep%20learning/regularization/normalization/2021/10/20/Dropout-and-Batch-Normalization.html",
            "relUrl": "/deep%20learning/regularization/normalization/2021/10/20/Dropout-and-Batch-Normalization.html",
            "date": " ‚Ä¢ Oct 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Configuring a Deep Learning system",
            "content": "For both CPU / GPU users . wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh bash Miniconda3-py39_4.9.2-Linux-x86_64.sh . Reboot. Test your installation using the command: conda list . sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential cmake unzip pkg-config sudo apt-get install gcc-7 g++-7 sudo apt-get install screen sudo apt-get install libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev sudo apt-get install libjpeg-dev libpng-dev libtiff-dev sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev sudo apt-get install libxvidcore-dev libx264-dev sudo apt-get install libopenblas-dev libatlas-base-dev liblapack-dev gfortran sudo apt-get install libhdf5-serial-dev sudo apt-get install python3-dev python3-tk python-imaging-tk sudo apt-get install libgtk-3-dev . Only for GPU users . Add NVIDIA package repositories . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo add-apt-repository &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /&quot; sudo apt-get update wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt-get update wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer7_7.1.3-1+cuda11.0_amd64.deb sudo apt install ./libnvinfer7_7.1.3-1+cuda11.0_amd64.deb sudo apt-get update . Install development and runtime libraries (~4GB) . sudo apt-get install --no-install-recommends cuda-11-0 libcudnn8=8.0.4.30-1+cuda11.0 libcudnn8-dev=8.0.4.30-1+cuda11.0 . Reboot. Check that GPUs are visible using the command: nvidia-smi . Install TensorRT. Requires that libcudnn8 is installed above. . sudo apt-get install -y --no-install-recommends libnvinfer7=7.1.3-1+cuda11.0 libnvinfer-dev=7.1.3-1+cuda11.0 libnvinfer-plugin7=7.1.3-1+cuda11.0 . For both CPU / GPU users . wget https://bootstrap.pypa.io/get-pip.py sudo python3 get-pip.py pip install numpy pip install tensorflow pip install opencv-contrib-python pip install scikit-image pip install pillow pip install imutils pip install scikit-learn pip install matplotlib pip install progressbar2 pip install beautifulsoup4 pip install pandas .",
            "url": "https://rishiraj.github.io/blog/deep%20learning/setup/2021/10/20/Configuring-a-Deep-Learning-system.html",
            "relUrl": "/deep%20learning/setup/2021/10/20/Configuring-a-Deep-Learning-system.html",
            "date": " ‚Ä¢ Oct 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Hello World of Machine Learning",
            "content": "Introduction . We will start at the very beginning: what exactly is ‚Äúmachine learning‚Äù, and how is it used in the real world? We‚Äôll learn the answers to these questions and explore the basics of decision trees, as we start to build a strong foundation for some of the most cutting-edge techniques in data science. . We‚Äôll learn all about pandas, the primary tool used by data scientists for exploring and manipulating data. Then, we‚Äôll use our new knowledge to examine a dataset of Chicken Tikka Masalas. . Who&#39;s ML? I&#39;ll do you one better, why is ML? . Let&#39;s pretend for a moment that there&#39;s a Chicken Tikka Masala cooking competition and we want to come up with a recipe that will get us the highest rating on a continuous 10-point scale. One approach would just be to use the recipe we have on hand and hope for the best but instead what if we put our newly acquired machine learning skills to the test. We can do this by finding hundreds, thousands or even millions of different Chicken Tikka Masala recipes and listing out all of their ingredients and corresponding Chicken Tikka Masala cooking competition ratings into a table like this where each row is a recipe and each column is the amount of ingredients in the given recipe. . tikkas . Rating Chicken Ginger Garlic Onion Tomato Garam Masala Salt Cream . 0 8.5 | 2.25 | 2 | 2 | 2.0 | 4 | 1 | 1.0 | 2 | . 1 1.0 | 1.50 | 6 | 1 | 1.0 | 0 | 0 | 12.0 | 0 | . 2 6.3 | 2.00 | 2 | 3 | 1.5 | 2 | 1 | 0.5 | 1 | . 3 5.0 | 4.50 | 1 | 5 | 5.0 | 3 | 1 | 3.0 | 0 | . 4 8.0 | 2.00 | 2 | 0 | 1.5 | 3 | 2 | 1.0 | 3 | . 5 7.0 | 2.00 | 1 | 1 | 0.5 | 4 | 1 | 0.0 | 4 | . Then we could fit a decision tree model to this dataset and use it to help predict the best combination of ingredients in order to make the highest rated Chicken Tikka Masala. What we&#39;re hoping to accomplish by doing this is to use the model we build to uncover or explain the relationship between the inputs (Chicken Tikka Masala ingredients) and the output (Chicken Tikka Masala rating). A common pattern that you&#39;re going to see is that we: . Define a model | Fit a model | Make predictions | Validate our model | When we define a model what you want to do is think to yourself, &quot;what model would I like to use?&quot; Of course, as you develop your machine learning skills, you&#39;ll have a wide array of models to choose from and you&#39;ll start to develop a sense as to which models will be best suited for any given task. If we want to think about this in more general terms we could abstractly represent a model using code that says my_model = ModelName(). . Fitting a model is another way to refer to training a model and essentially what we&#39;re doing is we&#39;re taking the model that we&#39;ve defined and we&#39;re applying that model to our dataset and we&#39;re asking it to start pulling out the underlying patterns in the data so an abstract code snippet for what fitting a model might look like is my_model.fit(features, target). . Making predictions is what happens once we&#39;ve built a model and we want to generalize it or extend it or apply it to data that it has never seen before and I know it&#39;s kind of silly to talk about what a model sees since technically it&#39;s a piece of code and it doesn&#39;t have eyes but just roll on me on this one. A general way to write making predictions would be something like my_model.predict(data). . Data Exploration . It&#39;s always good to know what you&#39;re getting into before you start something and one of the best ways to have an idea of what you&#39;re getting into is to ask a lot of questions. So let&#39;s think for a minute about our Chicken Tikka Masala problem, what we&#39;re trying to do is map a Chicken Tikka Masala recipe to its rating. So what we can hypothetically do is go out and collect all of this recipe data for Chicken Tikka Masalas and organize that data into something called a dataframe. Once we have our data organized into a dataframe we can begin to use some python code to ask and answer various questions about our data. Now you can ask almost an infinite number of questions but some of my favorite starting questions are things like, &quot;What are the names of the variables in my data?&quot;, &quot;How many variables are in my dataframe?&quot;, &quot;How many observations or rows are in my dataframe?&quot;. I can also ask things like, &quot;What do the first few rows of my dataframe look like?&quot;, &quot;Are there any missing values in my dataframe?&quot;. I can even ask, &quot;What&#39;s the average value of a numerical variable within my dataframe?&quot;. . Let&#39;s try answering some of these questions on our Chicken Tikka Masala dataset by using the Pandas library. So for example if we want to know how many observations and variables or rows and columns are in our dataframe, we can use tikkas.shape to see that we have x rows and y columns. . import pandas as pd tikkas = pd.read_csv(&quot;/content/tikka.csv&quot;) . tikkas.shape # There are 6 rows and 9 columns in the tikkas dataframe . (6, 9) . To look at the names of the variables in our tikkas dataframe we can run tikkas.columns. . tikkas.columns . Index([&#39;Rating&#39;, &#39;Chicken&#39;, &#39;Ginger&#39;, &#39;Garlic&#39;, &#39;Onion&#39;, &#39;Tomato&#39;, &#39;Garam Masala&#39;, &#39;Salt&#39;, &#39;Cream&#39;], dtype=&#39;object&#39;) . And if we want to look at the first few rows of our dataframe, well then we can use tikkas.head(). . tikkas.head() . Rating Chicken Ginger Garlic Onion Tomato Garam Masala Salt Cream . 0 8.5 | 2.25 | 2 | 2 | 2.0 | 4 | 1 | 1.0 | 2 | . 1 1.0 | 1.50 | 6 | 1 | 1.0 | 0 | 0 | 12.0 | 0 | . 2 6.3 | 2.00 | 2 | 3 | 1.5 | 2 | 1 | 0.5 | 1 | . 3 5.0 | 4.50 | 1 | 5 | 5.0 | 3 | 1 | 3.0 | 0 | . 4 8.0 | 2.00 | 2 | 0 | 1.5 | 3 | 2 | 1.0 | 3 | . What if there&#39;s missing data? We can explore that by running tikkas.isnull().sum(). . tikkas.isnull().sum() # There are no missing values in the tikkas dataframe . Rating 0 Chicken 0 Ginger 0 Garlic 0 Onion 0 Tomato 0 Garam Masala 0 Salt 0 Cream 0 dtype: int64 . We can also get a table with summary values by using tikkas.describe(). . tikkas.describe() . Rating Chicken Ginger Garlic Onion Tomato Garam Masala Salt Cream . count 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | 6.000000 | . mean 5.966667 | 2.375000 | 2.333333 | 2.000000 | 1.916667 | 2.666667 | 1.000000 | 2.916667 | 1.666667 | . std 2.732520 | 1.069462 | 1.861899 | 1.788854 | 1.594261 | 1.505545 | 0.632456 | 4.565267 | 1.632993 | . min 1.000000 | 1.500000 | 1.000000 | 0.000000 | 0.500000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 5.325000 | 2.000000 | 1.250000 | 1.000000 | 1.125000 | 2.250000 | 1.000000 | 0.625000 | 0.250000 | . 50% 6.650000 | 2.000000 | 2.000000 | 1.500000 | 1.500000 | 3.000000 | 1.000000 | 1.000000 | 1.500000 | . 75% 7.750000 | 2.187500 | 2.000000 | 2.750000 | 1.875000 | 3.750000 | 1.000000 | 2.500000 | 2.750000 | . max 8.500000 | 4.500000 | 6.000000 | 5.000000 | 5.000000 | 4.000000 | 2.000000 | 12.000000 | 4.000000 | . We&#39;ve covered a ton of information related to the basic data exploration. I hope you&#39;ve learned something in this blog post. I know I have and I&#39;ve really enjoyed learning with you. So when you&#39;re ready meet me in the next post where we&#39;re going to build our first Machine Learning model. .",
            "url": "https://rishiraj.github.io/blog/machine%20learning/data%20exploration/2021/08/09/Hello-World-of-Machine-Learning.html",
            "relUrl": "/machine%20learning/data%20exploration/2021/08/09/Hello-World-of-Machine-Learning.html",
            "date": " ‚Ä¢ Aug 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://rishiraj.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "ùî∏ùïìùï†ùï¶ùï• ùïÑùïñ",
          "content": "Hey there, I‚Äôm Rishiraj Acharya, just another boring, short, coke dependent, undergraduate Machine Learning Engineer who lives in Kolkata, India. You‚Äôre probably reading this for my contributions in tech, but interesting fact, I didn‚Äôt always want to get into tech. I was always a medical aspirant. However, my marks in entrance exams slapped me and said, ‚Äúyou‚Äôre made for tech‚Äù. I started my tech journey after getting into university and soon explored almost everything: from App Development to Web and from coffee to cookies. Failed in some, lost interest in some but finally I found my love: Machine Learning and chicken wings. . Since then I studied Machine Learning extensively focusing on Neural Networks, worked at companies to gain practical ML experience and made sure to utilize my time in helping my peers learn, connect and grow as a community. I was a Microsoft Student Partner, a deeplearning.ai Ambassador, a ML team lead at Google DSC and an active volunteer for Google Developer Groups Kolkata. I love taking sessions and speaking at various conferences for promoting student knowledge on developer products and platforms. I also taught Python to students during the pandemic as a facilitator at Stanford University. . These were some things that you can find about me on the internet, want a secret? I‚Äôm a national level Chess player, a swimming champion and I can lecture for hours on the outer reaches of space and the craziness of astrophysics. . Warning: If you‚Äôre a vegetarian and you‚Äôre easily offended with people discussing meaty food, this site may not be your favorite! .",
          "url": "https://rishiraj.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://rishiraj.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}